{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vaderSentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvaderSentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvaderSentiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, KFold, StratifiedKFold, GridSearchCV\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vaderSentiment'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"trumptweet.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tweets.columns: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.dropna(inplace = True) \n",
    "np.array([len(tweets.text) for tweet in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"Text_Length\"]= tweets[\"text\"].str.len() \n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have added code to figure out the length of each tweet in tweets\n",
    "\n",
    "There are outliers present; We have to remove the outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "tweets=tweets[np.abs(tweets.Text_Length-tweets.Text_Length.mean()) <= (3*tweets.Text_Length.std())]\n",
    "tweets.reset_index(inplace = True) \n",
    "tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_lengths = pd.Series(data=tweets['Text_Length'].values, index=tweets['created_at'])\n",
    "Tweet_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_lengths.plot(figsize=(16,4), color='r')\n",
    "plt.xlabel('Date of Tweet')\n",
    "plt.ylabel('Length of Tweet')\n",
    "plt.title(\"Length of Tweets by Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = []\n",
    "for source in tweets['source']:\n",
    "    if source not in sources:\n",
    "        sources.append(source)\n",
    "        \n",
    "print(\"Content sources:\")\n",
    "for source in sources:\n",
    "    print(\"* {}\".format(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = np.zeros(len(sources))\n",
    "\n",
    "for source in tweets['source']:\n",
    "    for index in range(len(sources)):\n",
    "        if source == sources[index]:\n",
    "            percent[index] += 1\n",
    "            pass\n",
    "\n",
    "percent /= 100\n",
    "\n",
    "# Pie chart:\n",
    "pie_chart = pd.Series(percent, index=sources, name='Sources')\n",
    "pie_chart.plot.pie(fontsize=11, autopct='%.4f', figsize=(6, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['created_at']= pd.to_datetime(tweets['created_at']) \n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Use re to remove special characers and keep only the necessary characters. \n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def analize_sentiment(tweet):\n",
    "    '''\n",
    "    Classify whether a tweet is positive or negative using polarity for cleaned tweets\n",
    "    '''\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cleaned'] = np.array([ clean_tweet(tweet) for tweet in tweets['text'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment'] = np.array([ analize_sentiment(tweet) for tweet in tweets['text'] ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = [ tweet for index, tweet in enumerate(tweets['text']) if tweets['sentiment'][index] > 0]\n",
    "neutral = [ tweet for index, tweet in enumerate(tweets['text']) if tweets['sentiment'][index] == 0]\n",
    "negative = [ tweet for index, tweet in enumerate(tweets['text']) if tweets['sentiment'][index] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive tweets: {}%\".format(len(positive)*100/len(tweets['text'])))\n",
    "print(\"Neutral tweets: {}%\".format(len(neutral)*100/len(tweets['text'])))\n",
    "print(\"Negative tweets: {}%\".format(len(negative)*100/len(tweets['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['date'] = tweets['created_at'].dt.date\n",
    "tweets[\"Date\"] = tweets['created_at'].dt.date\n",
    "\n",
    "tweets = tweets.groupby(by='date').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['count']= tweets['created_at']\n",
    "mean_tweets = tweets['count'].mean()\n",
    "print(mean_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After a small sneak peak, we shall begin the bulk of our analysis\n",
    "Let us start by changing the way we perform a sentiment analysis while also utilizing more data from Trump's Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"Data/trumptweets.csv\", header=0,encoding = 'unicode_escape') #Updated Tweet data\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock1 = pd.read_csv('Data/historical.csv')\n",
    "stock2 = pd.read_csv('Data/historical2.csv')\n",
    "stock3 = pd.read_csv('Data/historical3.csv')\n",
    "stock4 = pd.read_csv('Data/historical4.csv')\n",
    "stock5 = pd.read_csv('Data/historical5.csv')\n",
    "stock6 = pd.read_csv('Data/historical6.csv')\n",
    "stock_df = stock1.append([stock2,stock3,stock4,stock5,stock6])\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_df = stock_df[['Date', 'Open']]\n",
    "trim_df['pure_date'] = trim_df.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "change = []\n",
    "for item in stock_df.Change:\n",
    "    if item > 0:\n",
    "        change.append(1)\n",
    "    else:\n",
    "        change.append(-1)\n",
    "trim_df['Change'] = change\n",
    "trim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stock_df.plot('Date','Open')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "ax = sns.lineplot(x=trim_df.pure_date,y=trim_df.Open, linewidth=0.2,color='black')\n",
    "ax.set(title='S&P 500 Prices',\n",
    "       xlabel='Date for S&P 500',\n",
    "       xticks=['02/27/14', '02/14/15', '02/03/16', '01/22/17', '01/09/18'],\n",
    "      ylabel='Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tweets['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to use a sentiment classifier\n",
    "\n",
    "tweets['sentiment'] = vs['compound'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[['sentiment', 'subjectivity']] = tweets['text'].apply(lambda text: pd.Series(TextBlob(text).sentiment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "# tweets[\"sentimentscore\"] = tweets.apply(lambda row: sid.polarity_scores(row['text'])['compound'] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['date']= pd.to_datetime(tweets['created_at']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['just_date'] = tweets['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_df['date']= pd.to_datetime(trim_df['Date']) \n",
    "trim_df['just_date'] = trim_df['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined= pd.merge(trim_df, tweets, on='just_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This chunk of code below is optional, but it is so we can analyze the stock price the day after the tweet was sent.\n",
    "For our purpose, we shall observe changes/fluctuations on the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pure_date = []           \n",
    "# for item in tweets.date:\n",
    "#     try:\n",
    "#         pure_date.append(datetime.date(year=item.year, month=item.month, day=item.day+1))\n",
    "#     except:\n",
    "#         pure_date.append(datetime.date(year=item.year, month=item.month+1, day=1))\n",
    "#     else:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined.drop(['Date','Open','pure_date', 'date_x', 'text' , \"created_at\", 'date_y', 'just_date'], axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can Start the modelling aspect\n",
    "\n",
    "We shall use the processed dataframe from above to model several aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Change'], axis=1)\n",
    "y = df['Change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25, shuffle=False)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_set_1 = [LogisticRegression()]\n",
    "grid = {'C':np.logspace(-3,2,10)}\n",
    "for model in model_set_1:\n",
    "    gscv = GridSearchCV(model,param_grid=grid,n_jobs=-1,cv=10)\n",
    "    gscv.fit(X_train,y_train)\n",
    "    print(gscv.best_estimator_)\n",
    "    print(gscv.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_set_2 = KNeighborsClassifier()\n",
    "grid={'n_neighbors' : range(2,50,1)}\n",
    "gscv2 = GridSearchCV(model_set_2,param_grid=grid,n_jobs=-1,cv=10)\n",
    "gscv2.fit(X_train,y_train)\n",
    "print(gscv2.best_estimator_)\n",
    "print(gscv2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_set_3 = [DecisionTreeClassifier()]\n",
    "grid = {}\n",
    "for model in model_set_3:\n",
    "    gscv3 = GridSearchCV(model,param_grid=grid,n_jobs=-1,cv=10)\n",
    "    gscv3.fit(X_train,y_train)\n",
    "    print(gscv3.best_estimator_)\n",
    "    print(gscv3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained\n",
    "From the above cross validation tests and scores, we can clearly see that logistic regression performs the best, followed by the decision tree classifier, then followed by the KNN classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with a 10 fold cross validation gives us a score of 0.5269324938789787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier with cv of 10 gives us a score of 0.4863588667366212, less than a coin toss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier with a cv of 10 gives us a score of  0.48382301504022385 which is only very slightly under the Decision tree classifier. This is a little less than a coin toss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set evaluation of Trump on S&P 500 Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall test the simple Logistic Regression classifier on our test dataset. \n",
    "From our previous iterations on test set, we saw that Logistic Regression performed the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 0\n",
    "for i in np.logspace(-3,2,10):\n",
    "    logreg = LogisticRegression(C=i)\n",
    "    logreg.fit(X_train,y_train)\n",
    "    if logreg.score(X_test,y_test)>top:\n",
    "        top = logreg.score(X_test,y_test)\n",
    "        top_i = i\n",
    "print('LOGREG - C=', top_i)\n",
    "print('Accuracy:',top)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=0\n",
    "for i in range(2,50):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    if knn.score(X_test,y_test) > top:\n",
    "        top=knn.score(X_test,y_test)\n",
    "        top_i = i\n",
    "print(f'KNN with {top_i} Neighbors - Accuracy:', top)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)\n",
    "print('Decision Tree Accuracy:', dt.score(X_test,y_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we can see that the logistic regression function and model performed the best and is more than 10% accurate in comparison to a coin toss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further exploration\n",
    "On Kaggle and as described in lecture, we can see that the XGBoost model classifier is continually winning competitions and seems to be a good fit. \n",
    "\n",
    "Let us see if the algorithm can help us predict our data any better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "mat_list = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train1 = X.iloc[train_index]\n",
    "    y_train1 = y[train_index]\n",
    "    xgb_model = xgb.XGBClassifier().fit(X_train1,y_train1)\n",
    "    predictions = xgb_model.predict(X.iloc[test_index])\n",
    "    actuals = y[test_index]\n",
    "    mat_list.append(confusion_matrix(actuals, predictions))\n",
    "matrix = mat_list[0]\n",
    "for n_mat in range(1,5):\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            matrix[i][j] += mat_list[n_mat][i][j]\n",
    "true = matrix[0][0] + matrix[1][1]\n",
    "false = matrix[1][0] + matrix[0][1]\n",
    "print('XGB performance', true/(true+false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier().fit(X_train,y_train)\n",
    "predictions = xgb_model.predict(X_test)\n",
    "actuals = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGB Accuracy', xgb_model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(actuals, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
